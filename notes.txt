# algorithms
Algorithms in java examples

https://www.geeksforgeeks.org/searching-algorithms/

Time complexity :
https://www.geeksforgeeks.org/analysis-of-algorithms-set-4-analysis-of-loops/
https://en.wikipedia.org/wiki/Time_complexity
https://discrete.gr/complexity/
http://www.bigocheatsheet.com

Terms
Complexity analysis is a tool that allows us to explain how an algorithm behaves as it's input grows larger or scales (n is the input size).
In order to determine the complexity of an algorithm we need to calculate the number of instructions as a function of n,
where n is the number of times it executes (f(n)= 6 + 2n) in the worst cases (Worst-case analysis).
Asymptomatic behavior is the process of taking the function that resulted from counting the number of
instructions through a filter, ignoring the processing power of the CPU, programming language details, etc.
Besides this kind of details, we have to remove the constant part of the function and
keep the one that grow the fastest (if we have multiple n). (E.g f(n)=6n+2 becomes f(n)=n)
Mathematically speaking, what we're saying here is that we're interested in the limit of function f as n tends to infinity.


f( n ) = n + 12 -> f(n) = n
f( n ) = 3 ^n + 2n -> f(n) = 3 ^n (3 ^ n is bigger that 2n)
f( n ) = 100 -> f(n) = 1


Generally for small programs we can conclude :

If we have a program without any loops then the asymptomatic behavior will be f(n) = 1,
because the number of instructions will always be constant(we need to add 1 if there is no n; we cannot put 0 because
the function should return non zero value).
If we have 1 loop(no recursion) then the asymptomatic behavior will always be f(n)=n.
For each loop that we add inside another loop there will f(n)=n ^(number of loops)
If we have multiple sequential for loops, the slowest of them dominates the asymptomatic behaviour.

Time complexity is the number of steps taken for an algorithm to complete. It usually takes the worst case scenario in order to calculate it's time complexity
Space (Memory) complexity is the amount of memory an algorithm uses
Time complexity uses Big O notation.
Time complexity (algorithm complexity) for f(n)=3n + 12 -> f(n)=n is the asymptomatic behavior noted as O(n).
Time complexity is not always mathematically exact. It just gives you an idea.
Rule of thumb: Programs with bigger O run slower than programs with smaller O

Best to Worst
--------------------------------------
O(1) is constant time algorithm      |
O(log(n)) is logarithmic algorithm   |
O(n) is linear algorithm             |
O(nlog(n)) is n log-start n          |
O(n ^2) is quadratic algorithm       |
--------------------------------------

In order to calculate complex algorithms we have to make them worse in order arrive at one of the general rules mentioned above.



Logarithm  = the inverse of exponentiation. It uses bases. For algorithm complexity it uses base 2 (binary logarithm)

exponentiation -> 2^3 = 8
logarithm -> log2(8) = 3 because 2 ^ 3  = 8

Basically the question is at what power should we raise number 2 in order to get 8. The result of logarithm is the actual power.
Logarithm of a given number x is the exponent to which another fixed number, the base b, must be raised, to produce that number x

log b (x) = y  if b^y = x


-----------------------
DATA STRUCTURES

1. Arrays

-uses a contiguous block in memory (elements are not scattered around the memory)
-every element occupies the same amount of space in memory
-arrays have each of their elements occupy the amount of memory needed for that data type.
-array of objects store references not objects
-if an array stats at memory address X, and the size of each element in the array is Y, we can calculate the memory address of
the ith element by using the following expression : X + i * Y
-if we know the index of an element , the time to retrieve the element will be the same, no matter where it is in an array.

get by index is O(1)
get when index is unknown is O(n) because we have to search the array, and in worst case we have to search the whole array
add element toa full array is O(n) because we have to create a new array and copy all the items to the new array
add element to the the end of array(has space) is O(1)
insert or update element at specific index is O(1)
delete an element by setting it to null is O(1) if we know the index, O(n) if we don't know the index
delete an element by shifting elements is O(n)

Essentially if we have to loop over the array we will have a O(n) time complexity, else it will be O(1)






SORTING
-------

Unstable sort is a sort that will not preserve the order of elements that are equal
Stable sort preservers the order of elelents that are equal
Example:

[1,3,4,5,4]

If the first 4 will still be before the second 4 after a sort, than it is a stable sort. For numbers it does not matter,
but for objects it might. If we do a sort within a sort, the second sort might break the whole sort if unstable. (ex: sort by name and than by age)

[ (John, 20) , (Eric, 30), (John, 40) ]

first sort is stable = [ (Eric, 30), (John, 20) , (John, 40) ]

second sort is unstable (moves the elements even though they were sorted ok)
and takes the result of the first sort = [ (Eric, 30), (John, 40), (John, 20)  ]